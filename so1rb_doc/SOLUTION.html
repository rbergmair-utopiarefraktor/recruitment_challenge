<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Dr Richard Bergmair" />
  <title>RB’s Approach To SO1’s Algorithm Challenge using Ideas from Bayesian and k-Nearest-Neighbor Classification</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">RB’s Approach To SO1’s Algorithm Challenge using Ideas from Bayesian and k-Nearest-Neighbor Classification</h1>
<h2 class="author">Dr Richard Bergmair</h2>
<h3 class="date">Oct-10 2015</h3>
</div>
<h1 id="preface">Preface</h1>
<h2 id="notes-on-style">Notes on Style</h2>
<p>So, this is my report on my approach to the algorithm challenge. From my work with one of my previous clients, I’m quite used to doing a lot of communication in written form and over the internet. In particular, we were using a ticketing system with a markup language in order to document progress on the various tasks and projects we were working on, as well as solicit comments and facilitate discussion. I’ve decided to write this document in a similar sort of style, writing it essentially the way I would write an e-mail to a person who is well-known to me, who is an expert on the subject matter under discussiona and who I would assume to give a generally sympathetic reading to the document.</p>
<p>If you’re trying to form an opinion on whether I can rise to the standard of rigor common among scientists for published work, have a look at my web page at <a href="http://richard.bergmair.eu/">richard.bergmair.eu</a> for a proper publication list.</p>
<h2 id="the-revolution-will-not-be-peer-reviewed">The Revolution Will Not Be Peer-Reviewed</h2>
<p>I should stress that one of the features of this more informal style is the general absence of citations and references of the kind that academics are used to. Wherever I actually use any outside resources or any outside documentation, or whenever I have any pointers that I think might be helpful or interesting to the reader, I do link to them.</p>
<p>But if, throughout this document, a reader forms the opinion that I am expressing an idea that really, really, has a canonical reference that should always go with it, and I don’t put it in, then it’s not for sheer ignorance, but rather, it is a conscious decision on my part not to play that game.</p>
<p>For example, when I was finished with this software, and started writing the report, I pondered slapping a label on it, such as “Bayesian k Nearest Neighbors”. Putting that into Google, it turns out that someone has already used the term as the title of <a href="http://www.people.fas.harvard.edu/~junliu/Workshops/workshop2007/talkSlides/ChristianRobert_knn.pdf">some slideshow</a>. As an academic, the onus would now be on me, to try to figure out what they did, how that relates to what I did, and then pretend like I was aware of their work and applied it (even when there was independent discovery), or pretend like I was aware of their work and saw a problem with it, justifying why I didn’t apply it, but when the solution to the problem is already on the table, then IMHO it’s not, in general, a good use of time to put a lot of work into sorting out who invented the various ideas that ended up playing a role in that solution. A certain level of reflectiveness is however useful. For example, when I find myself using an idea a lot, I sometimes get sufficiently curious to decide I want to make an effort to find out what it’s called, who invented it, and what other people have said about it, etc. But what I’m trying to say is that this is not a matter of principle, but rather that there needs to be a conscious decision about whether or not that’s a good use of time in any particular case, especially in light of the fact that fly-fishing is a good use of time, too.</p>
<h2 id="more-pseudo-philosophical-waffle">More Pseudo-Philosophical Waffle</h2>
<p>Wearing my partitioner’s hat, what I do is to apply the inventory of ideas and techniques that my academic alter ego has picked up through the years to try to understand the logic and the structure of the problem I’m trying to solve. That understanding more or less dictates a solution which is the simplest solution to the problem. If that solution is novel, then great. If that solution is on page one of every textbook on the subject, then so be it. – If you’re a software developer, and you’re confronted with a problem that calls for a solution that is novel, and you can’t see it, because you lack the conceptual inventory, then that’s weak. If you’re a dysfunctional academic peddling a solution looking for a problem, and as a result, you end up hallucinating complexity where none exists, then that’s just as weak. (See <a href="https://www.youtube.com/watch?v=9RKlJ2oBROA&amp;NR=1">Gsellmann’s Weltmaschine</a>). Okay, so that’s that.</p>
<h2 id="solution-vs.exploration">Solution vs. Exploration</h2>
<p>In this document, I will outline the solution to the SO1 algorithm challenge that I came up with. The purpose of the present document (<code>SOLUTION</code>) is to tell you where that solution is located, and give you directions on how to get there most efficiently, starting from first principles. – There is a separte document (<code>EXPLORATION</code>) that deals with how I came to discover that route by taking lots and lots of roads that ended up being dead ends, applying lots and lots of analogies to problems I’ve dealt with in the past that sometimes did, but often didn’t apply here. – A lot of readers won’t be interested in reading about dead ends and about similar problems that ended up not being helpful in solving this one. So it makes sense to put that in a separate document.</p>
<p>Incidentally, the same structure exists in the code. There are two separate Python projects (<code>so1rb</code> and <code>so1rb_explore</code>). The project <code>so1rb_explore</code> is like “brainstorming” or like a “stream of consciousness” stage in drafting some piece of creative writing. It is a sequence of self-contained scripts, each of which answers a simple question about the data. They do not share any common structure, but rather they come about through a process of copying and pasting, which would not normally be proper software engineering. But the overarching idea here is that you’re trying to concentrate on the structure and the phenomena that are in the data, not on the structure of the code, so you allow yourself to forget about proper software engineering for the purpose of this exploration, so as not to be distracted.</p>
<p>Then you start with a clean slate, and create a well-structured and reusable piece of software that takes into account all and only those phenomena actually exhibited by the data, with the structure of the code informed by your knowledge of the structure of the data. And that’s what <code>so1rb</code> is. – If you skip straight to writing the code you’re intending to be the ultimate solution to the problem, then your starting point will essentially be a shot in the dark. As you try to improve upon a suboptimal situation, you’ll find yourself trying to add illumination by putting in lots of parameters, switches, and debug-level code. But at that stage, a lot of very basic design choices will already have been made in a suboptimal way, and once you’ve set all of your parameters and switches and deactivated all your debug code, you’ll end up with lots of orphaned code and unnecessary complexity that makes it hard for people to read and understand and hence to reuse that code in the future.</p>
<h1 id="structure-of-the-data-feature-engineering">Structure of the Data &amp; Feature Engineering</h1>
<p>Let’s finally delve into the problem at hand, and start talking about the data. It’s a CSV file, with the following columns</p>
<ul>
<li><span class="math">\(\mathrm{id}\)</span> (which is also called <span class="math">\(\mathtt{id}\)</span> in the data) is, well, the identifier. (You guessed that, didn’t you?)</li>
<li><span class="math">\(\mathrm{y}\)</span> is the dependent variable, i.e. the class label we are trying to predict as part of the classification problem at hand.</li>
<li><span class="math">\(\mathrm{cid}\)</span> (which is also called <span class="math">\(\mathtt{cid}\)</span> in the data) is a number in the range <span class="math">\([1,30]\)</span>. In combination with <a href="https://github.com/Segment-of-One/recruitment_challenge/issues/3">this comment here</a>, according to which this feature “somewhat structures the data”, I’m assuming that the abbreviation is for “category id”, and that the feature is not to be interpreted as a number, neither ordinal nor cardinal, but rather as a discrete symbol.</li>
<li>Among the columns called <span class="math">\(\mathtt{x}_1 \ldots \mathtt{x}_{100}\)</span> in the data, there are 70 continuous-valued numeric columns, and 30 columns with values <code>0</code> or <code>1</code>. In what follows, I’ll reindex these, so that <span class="math">\(\mathrm{x}_i\)</span> is the <span class="math">\(i\)</span>-th of the continuous-valued features, and <span class="math">\(\mathrm{b}_j\)</span> is the <span class="math">\(j\)</span>-th of the binary features (starting the indexing at one, in both cases).</li>
</ul>
<p>The module <code>so1rb/so1rb_data/da_read.py</code> implements a reader that reads this data format.</p>
<p>Throughout the rest of this section, I will describe the modules under <code>so1rb_frontend</code>. These are classes derived from <code>Frontend</code>, and they all serve the purpose of preprocessing the data so as to present it to the core classification algorithm in a suitable representation.</p>
<p>There are two stages to this:</p>
<ul>
<li>Decorrelation (= orthogonalization): So as to transform the data into a space in which features are as close as possible to pairwise mutually independent.</li>
<li>Feature selection: So as to identify and remove any components in the decorrelated (orthogonalized) space, that are unrelated to the dependent variable <span class="math">\(\mathrm{y}\)</span>.</li>
</ul>
<p>Within the decorrelation stage:</p>
<ul>
<li><code>CategoricalFrontend</code> is a structural placeholder (i.e. dummy class) that passes through the value of <span class="math">\(\mathrm{cid}\)</span> unchanged.</li>
<li><code>BinaryFrontend</code> uses the correlation structure among binary features to create categorical variables grouping together clusters of binary features that are strongly correlated with each other, but uncorrelated to features in other clusters.</li>
<li><code>HomebrewContinuousFrontend</code> applies the same idea to continuous variables. It uses the correlation structure among continuous features to create a smaller number of continuous features which are averages across clusters of continuous features that are strongly correlated with each other, but uncorrelated to features in other clusters.</li>
<li><code>KPCAContinuousFrontend</code> is a (more or less) drop-in replacement for <code>HomebrewContinuousFrontend</code> that instead uses <a href="http://scikit-learn.org/">scikit-learn</a>’s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html">Kernel PCA</a> method to digest the continuous features into a set of three continuous features.</li>
</ul>
<p>Within the feature selection stage:</p>
<ul>
<li><code>FeatureDiscretizer</code> turns the continuous-valued features which are the output of <code>HomebrewContinuousFrontend</code> into categorical variables by bucketizing them into 32 buckets of equal probabilitistic mass, i.e. first to third percentile, fourth to sixth percentile, etc. This is not used for classification, but only as a preprocessing step for the <code>FeatureSelector</code> (described below), and to speed up lookups for k-nearest-neighbor, but indexing on descretized values.</li>
<li><code>FeatureSelector</code> uses categorical features, including discretized continuous features, to look at the correlation structure and eliminate features that seem unrelated to the dependent variable <span class="math">\(\mathrm{y}\)</span>.</li>
</ul>
<p>The following schematic shows how those might interact with each other, in an example-setting in which there are only 10, instead of 100, features.</p>
<div style="width:25em">
<img src="Fig1.svg" alt="Fig1" />
</div>
<p>In what follows, I’ll cover each of the frontend components in turn, describing how they work, and what benefit they achieve in terms of orthogonalization and dimensionality reduction.</p>
<h2 id="binaryfrontend"><code>BinaryFrontend</code></h2>
<p>A set of <span class="math">\(30\)</span> binary features yields <span class="math">\(2^{30} \approx 1\mathrm{G}\)</span> combinations of values.</p>
</body>
</html>
